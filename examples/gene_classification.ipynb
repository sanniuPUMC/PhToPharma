{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geneformer Fine-Tuning for Classification of Dosage-Sensitive vs. -Insensitive Transcription Factors (TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU_NUMBER = [0]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(s) for s in GPU_NUMBER])\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import datetime\n",
    "import subprocess\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from geneformer import DataCollatorForGeneClassification\n",
    "from geneformer.pretrainer import token_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gene Attribute Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of corresponding Ensembl IDs, gene names, and gene types (e.g. coding, miRNA, etc.)\n",
    "gene_info = pd.read_csv(\"/path/to/gene_info_table.csv\", index_col=0)\n",
    "\n",
    "# create dictionaries for corresponding attributes\n",
    "gene_id_type_dict = dict(zip(gene_info[\"ensembl_id\"],gene_info[\"gene_type\"]))\n",
    "gene_name_id_dict = dict(zip(gene_info[\"gene_name\"],gene_info[\"ensembl_id\"]))\n",
    "gene_id_name_dict = {v: k for k,v in gene_name_id_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data and Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preparing targets and labels\n",
    "def prep_inputs(genegroup1, genegroup2, id_type):\n",
    "    if id_type == \"gene_name\":\n",
    "        targets1 = [gene_name_id_dict[gene] for gene in genegroup1 if gene_name_id_dict.get(gene) in token_dictionary]\n",
    "        targets2 = [gene_name_id_dict[gene] for gene in genegroup2 if gene_name_id_dict.get(gene) in token_dictionary]\n",
    "    elif id_type == \"ensembl_id\":\n",
    "        targets1 = [gene for gene in genegroup1 if gene in token_dictionary]\n",
    "        targets2 = [gene for gene in genegroup2 if gene in token_dictionary]\n",
    "            \n",
    "    targets1_id = [token_dictionary[gene] for gene in targets1]\n",
    "    targets2_id = [token_dictionary[gene] for gene in targets2]\n",
    "    \n",
    "    targets = np.array(targets1_id + targets2_id)\n",
    "    labels = np.array([0]*len(targets1_id) + [1]*len(targets2_id))\n",
    "    nsplits = min(5, min(len(targets1_id), len(targets2_id))-1)\n",
    "    assert nsplits > 2\n",
    "    print(f\"# targets1: {len(targets1_id)}\\n# targets2: {len(targets2_id)}\\n# splits: {nsplits}\")\n",
    "    return targets, labels, nsplits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing targets and labels for dosage sensitive vs insensitive TFs\n",
    "dosage_tfs = pd.read_csv(\"/path/to/dosage_sens_tf_labels.csv\", header=0)\n",
    "sensitive = dosage_tfs[\"dosage_sensitive\"].dropna()\n",
    "insensitive = dosage_tfs[\"dosage_insensitive\"].dropna()\n",
    "targets, labels, nsplits = prep_inputs(sensitive, insensitive, \"ensembl_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "train_dataset=load_from_disk(\"/path/to/gene_train_data.dataset\")\n",
    "shuffled_train_dataset = train_dataset.shuffle(seed=42)\n",
    "subsampled_train_dataset = shuffled_train_dataset.select([i for i in range(50_000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Training and Cross-Validating Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_classifier_batch(cell_batch):\n",
    "    max_batch_len = max([len(i) for i in cell_batch[\"input_ids\"]])\n",
    "    def pad_label_example(example):\n",
    "        example[\"labels\"] = np.pad(example[\"labels\"], \n",
    "                                   (0, max_batch_len-len(example[\"input_ids\"])), \n",
    "                                   mode='constant', constant_values=-100)\n",
    "        example[\"input_ids\"] = np.pad(example[\"input_ids\"], \n",
    "                                      (0, max_batch_len-len(example[\"input_ids\"])), \n",
    "                                      mode='constant', constant_values=token_dictionary.get(\"<pad>\"))\n",
    "        example[\"attention_mask\"] = (example[\"input_ids\"] != token_dictionary.get(\"<pad>\")).astype(int)\n",
    "        return example\n",
    "    padded_batch = cell_batch.map(pad_label_example)\n",
    "    return padded_batch\n",
    "\n",
    "# forward batch size is batch size for model inference (e.g. 200)\n",
    "def classifier_predict(model, evalset, forward_batch_size, mean_fpr):\n",
    "    predict_logits = []\n",
    "    predict_labels = []\n",
    "    model.eval()\n",
    "    for i in range(0, len(evalset), forward_batch_size):\n",
    "        max_range = min(i+forward_batch_size,len(evalset))\n",
    "        batch_evalset = evalset.select([i for i in range(i, max_range)])\n",
    "        padded_batch = preprocess_classifier_batch(batch_evalset)\n",
    "        padded_batch.set_format(type=\"torch\")\n",
    "        \n",
    "        input_data_batch = padded_batch[\"input_ids\"]\n",
    "        attn_msk_batch = padded_batch[\"attention_mask\"]\n",
    "        label_batch = padded_batch[\"labels\"]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids = input_data_batch.to(\"cuda\"), \n",
    "                attention_mask = attn_msk_batch.to(\"cuda\"), \n",
    "                labels = label_batch.to(\"cuda\"), \n",
    "            )\n",
    "            predict_logits += [torch.squeeze(outputs.logits.to(\"cpu\"))]\n",
    "            predict_labels += [torch.squeeze(label_batch.to(\"cpu\"))]\n",
    "            \n",
    "    logits_by_cell = torch.cat(predict_logits)\n",
    "    all_logits = logits_by_cell.reshape(-1, logits_by_cell.shape[2])\n",
    "    labels_by_cell = torch.cat(predict_labels)\n",
    "    all_labels = torch.flatten(labels_by_cell)\n",
    "    logit_label_paired = [item for item in list(zip(all_logits.tolist(), all_labels.tolist())) if item[1]!=-100]\n",
    "    y_pred = [vote(item[0]) for item in logit_label_paired]\n",
    "    y_true = [item[1] for item in logit_label_paired]\n",
    "    logits_list = [item[0] for item in logit_label_paired]\n",
    "    # probability of class 1\n",
    "    y_score = [py_softmax(item)[1] for item in logits_list]\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    # plot roc_curve for this split\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC')\n",
    "    plt.show()\n",
    "    # interpolate to graph\n",
    "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    return fpr, tpr, interp_tpr, conf_mat \n",
    "\n",
    "def vote(logit_pair):\n",
    "    a, b = logit_pair\n",
    "    if a > b:\n",
    "        return 0\n",
    "    elif b > a:\n",
    "        return 1\n",
    "    elif a == b:\n",
    "        return \"tie\"\n",
    "    \n",
    "def py_softmax(vector):\n",
    "\te = np.exp(vector)\n",
    "\treturn e / e.sum()\n",
    "    \n",
    "# get cross-validated mean and sd metrics\n",
    "def get_cross_valid_metrics(all_tpr, all_roc_auc, all_tpr_wt):\n",
    "    wts = [count/sum(all_tpr_wt) for count in all_tpr_wt]\n",
    "    print(wts)\n",
    "    all_weighted_tpr = [a*b for a,b in zip(all_tpr, wts)]\n",
    "    mean_tpr = np.sum(all_weighted_tpr, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    all_weighted_roc_auc = [a*b for a,b in zip(all_roc_auc, wts)]\n",
    "    roc_auc = np.sum(all_weighted_roc_auc)\n",
    "    roc_auc_sd = math.sqrt(np.average((all_roc_auc-roc_auc)**2, weights=wts))\n",
    "    return mean_tpr, roc_auc, roc_auc_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate gene classifier\n",
    "def cross_validate(data, targets, labels, nsplits, subsample_size, training_args, freeze_layers, output_dir, num_proc):\n",
    "    # check if output directory already written to\n",
    "    # ensure not overwriting previously saved model\n",
    "    model_dir_test = os.path.join(output_dir, \"ksplit0/models/pytorch_model.bin\")\n",
    "    if os.path.isfile(model_dir_test) == True:\n",
    "        raise Exception(\"Model already saved to this directory.\")\n",
    "    \n",
    "    # initiate eval metrics to return\n",
    "    num_classes = len(set(labels))\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    all_roc_auc = []\n",
    "    all_tpr_wt = []\n",
    "    label_dicts = []\n",
    "    confusion = np.zeros((num_classes,num_classes))\n",
    "    \n",
    "    # set up cross-validation splits\n",
    "    skf = StratifiedKFold(n_splits=nsplits, random_state=0, shuffle=True)\n",
    "    # train and evaluate\n",
    "    iteration_num = 0\n",
    "    for train_index, eval_index in tqdm(skf.split(targets, labels)):\n",
    "        if len(labels) > 500:\n",
    "            print(\"early stopping activated due to large # of training examples\")\n",
    "            nsplits = 3\n",
    "            if iteration_num == 3:\n",
    "                break\n",
    "        print(f\"****** Crossval split: {iteration_num}/{nsplits-1} ******\\n\")\n",
    "        # generate cross-validation splits\n",
    "        targets_train, targets_eval = targets[train_index], targets[eval_index]\n",
    "        labels_train, labels_eval = labels[train_index], labels[eval_index]\n",
    "        label_dict_train = dict(zip(targets_train, labels_train))\n",
    "        label_dict_eval = dict(zip(targets_eval, labels_eval))\n",
    "        label_dicts += (iteration_num, targets_train, targets_eval, labels_train, labels_eval)\n",
    "        \n",
    "        # function to filter by whether contains train or eval labels\n",
    "        def if_contains_train_label(example):\n",
    "            a = label_dict_train.keys()\n",
    "            b = example['input_ids']\n",
    "            return not set(a).isdisjoint(b)\n",
    "\n",
    "        def if_contains_eval_label(example):\n",
    "            a = label_dict_eval.keys()\n",
    "            b = example['input_ids']\n",
    "            return not set(a).isdisjoint(b)\n",
    "        \n",
    "        # filter dataset for examples containing classes for this split\n",
    "        print(f\"Filtering training data\")\n",
    "        trainset = data.filter(if_contains_train_label, num_proc=num_proc)\n",
    "        print(f\"Filtered {round((1-len(trainset)/len(data))*100)}%; {len(trainset)} remain\\n\")\n",
    "        print(f\"Filtering evalation data\")\n",
    "        evalset = data.filter(if_contains_eval_label, num_proc=num_proc)\n",
    "        print(f\"Filtered {round((1-len(evalset)/len(data))*100)}%; {len(evalset)} remain\\n\")\n",
    "\n",
    "        # minimize to smaller training sample\n",
    "        training_size = min(subsample_size, len(trainset))\n",
    "        trainset_min = trainset.select([i for i in range(training_size)])\n",
    "        eval_size = min(training_size, len(evalset))\n",
    "        half_training_size = round(eval_size/2)\n",
    "        evalset_train_min = evalset.select([i for i in range(half_training_size)])\n",
    "        evalset_oos_min = evalset.select([i for i in range(half_training_size, eval_size)])\n",
    "        \n",
    "        # label conversion functions\n",
    "        def generate_train_labels(example):\n",
    "            example[\"labels\"] = [label_dict_train.get(token_id, -100) for token_id in example[\"input_ids\"]]\n",
    "            return example\n",
    "\n",
    "        def generate_eval_labels(example):\n",
    "            example[\"labels\"] = [label_dict_eval.get(token_id, -100) for token_id in example[\"input_ids\"]]\n",
    "            return example\n",
    "        \n",
    "        # label datasets \n",
    "        print(f\"Labeling training data\")\n",
    "        trainset_labeled = trainset_min.map(generate_train_labels)\n",
    "        print(f\"Labeling evaluation data\")\n",
    "        evalset_train_labeled = evalset_train_min.map(generate_eval_labels)\n",
    "        print(f\"Labeling evaluation OOS data\")\n",
    "        evalset_oos_labeled = evalset_oos_min.map(generate_eval_labels)\n",
    "        \n",
    "        # create output directories\n",
    "        ksplit_output_dir = os.path.join(output_dir, f\"ksplit{iteration_num}\")\n",
    "        ksplit_model_dir = os.path.join(ksplit_output_dir, \"models/\") \n",
    "        \n",
    "        # ensure not overwriting previously saved model\n",
    "        model_output_file = os.path.join(ksplit_model_dir, \"pytorch_model.bin\")\n",
    "        if os.path.isfile(model_output_file) == True:\n",
    "            raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "        # make training and model output directories\n",
    "        subprocess.call(f'mkdir {ksplit_output_dir}', shell=True)\n",
    "        subprocess.call(f'mkdir {ksplit_model_dir}', shell=True)\n",
    "        \n",
    "        # load model\n",
    "        model = BertForTokenClassification.from_pretrained(\n",
    "            \"/path/to/pretrained_model/\",\n",
    "            num_labels=2,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False\n",
    "        )\n",
    "        if freeze_layers is not None:\n",
    "            modules_to_freeze = model.bert.encoder.layer[:freeze_layers]\n",
    "            for module in modules_to_freeze:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "        model = model.to(\"cuda:0\")\n",
    "        \n",
    "        # add output directory to training args and initiate\n",
    "        training_args[\"output_dir\"] = ksplit_output_dir\n",
    "        training_args_init = TrainingArguments(**training_args)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args_init,\n",
    "            data_collator=DataCollatorForGeneClassification(),\n",
    "            train_dataset=trainset_labeled,\n",
    "            eval_dataset=evalset_train_labeled\n",
    "        )\n",
    "\n",
    "        # train the gene classifier\n",
    "        trainer.train()\n",
    "        \n",
    "        # save model\n",
    "        trainer.save_model(ksplit_model_dir)\n",
    "        \n",
    "        # evaluate model\n",
    "        fpr, tpr, interp_tpr, conf_mat = classifier_predict(trainer.model, evalset_oos_labeled, 200, mean_fpr)\n",
    "        \n",
    "        # append to tpr and roc lists\n",
    "        confusion = confusion + conf_mat\n",
    "        all_tpr.append(interp_tpr)\n",
    "        all_roc_auc.append(auc(fpr, tpr))\n",
    "        # append number of eval examples by which to weight tpr in averaged graphs\n",
    "        all_tpr_wt.append(len(tpr))\n",
    "        \n",
    "        iteration_num = iteration_num + 1\n",
    "        \n",
    "    # get overall metrics for cross-validation\n",
    "    mean_tpr, roc_auc, roc_auc_sd = get_cross_valid_metrics(all_tpr, all_roc_auc, all_tpr_wt)\n",
    "    return all_roc_auc, roc_auc, roc_auc_sd, mean_fpr, mean_tpr, confusion, label_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "def plot_ROC(bundled_data, title):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    for roc_auc, roc_auc_sd, mean_fpr, mean_tpr, sample, color in bundled_data:\n",
    "        plt.plot(mean_fpr, mean_tpr, color=color,\n",
    "                 lw=lw, label=\"{0} (AUC {1:0.2f} $\\pm$ {2:0.2f})\".format(sample, roc_auc, roc_auc_sd))\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "# plot confusion matrix\n",
    "def plot_confusion_matrix(classes_list, conf_mat, title):\n",
    "    display_labels = []\n",
    "    i = 0\n",
    "    for label in classes_list:\n",
    "        display_labels += [\"{0}\\nn={1:.0f}\".format(label, sum(conf_mat[:,i]))]\n",
    "        i = i + 1\n",
    "    display = ConfusionMatrixDisplay(confusion_matrix=preprocessing.normalize(conf_mat, norm=\"l1\"), \n",
    "                                     display_labels=display_labels)\n",
    "    display.plot(cmap=\"Blues\",values_format=\".2g\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune With Gene Classification Learning Objective and Quantify Predictive Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "# max input size\n",
    "max_input_size = 2 ** 11  # 2048\n",
    "\n",
    "# set training parameters\n",
    "# max learning rate\n",
    "max_lr = 5e-5\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 4\n",
    "# number gpus\n",
    "num_gpus = 1\n",
    "# number cpu cores\n",
    "num_proc = 24\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 12\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"linear\"\n",
    "# warmup steps\n",
    "warmup_steps = 500\n",
    "# number of epochs\n",
    "epochs = 1\n",
    "# optimizer\n",
    "optimizer = \"adamw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set training arguments\n",
    "subsample_size = 10_000\n",
    "training_args = {\n",
    "    \"learning_rate\": max_lr,\n",
    "    \"do_train\": True,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 100,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"lr_scheduler_type\": lr_schedule_fn,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "    \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "    \"num_train_epochs\": epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output directory path\n",
    "current_date = datetime.datetime.now()\n",
    "datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "training_output_dir = f\"/path/to/models/{datestamp}_geneformer_GeneClassifier_dosageTF_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_n{subsample_size}_F{freeze_layers}/\"\n",
    "\n",
    "# ensure not overwriting previously saved model\n",
    "ksplit_model_test = os.path.join(training_output_dir, \"ksplit0/models/pytorch_model.bin\")\n",
    "if os.path.isfile(ksplit_model_test) == True:\n",
    "    raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "# make output directory\n",
    "subprocess.call(f'mkdir {training_output_dir}', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross-validate gene classifier\n",
    "all_roc_auc, roc_auc, roc_auc_sd, mean_fpr, mean_tpr, confusion, label_dicts \\\n",
    "    = cross_validate(subsampled_train_dataset, targets, labels, nsplits, subsample_size, training_args, freeze_layers, training_output_dir, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle data for plotting\n",
    "bundled_data = []\n",
    "bundled_data += [(roc_auc, roc_auc_sd, mean_fpr, mean_tpr, \"Geneformer\", \"red\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "plot_ROC(bundled_data, 'Dosage Sensitive vs Insensitive TFs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "classes_list = [\"Dosage Sensitive\", \"Dosage Insensitive\"]\n",
    "plot_confusion_matrix(classes_list, confusion, \"Geneformer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "eba1599a1f7e611c14c87ccff6793920aa63510b01fc0e229d6dd014149b8829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
